{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 1: Setup and Requirements ---\n",
        "!pip install pandas numpy scikit-learn xgboost requests joblib\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "import joblib\n",
        "import time\n",
        "\n",
        "# --- Configuration (UPDATE THIS!) ---\n",
        "# You need a key from OpenWeatherMap or a similar service to fetch real-time data.\n",
        "WEATHER_API_KEY = \"5197fc88f5f846ee7566eb28d403c91f\"\n",
        "THRESHOLD_MIN = 10 # Delivery is \"Late\" if actual time > estimated time + THRESHOLD_MIN\n",
        "\n",
        "def setup_environment():\n",
        "    \"\"\"Confirms environment setup.\"\"\"\n",
        "    print(\"Environment setup check completed. Required libraries are installed.\")\n",
        "\n",
        "# Run setup\n",
        "setup_environment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUSQovuJdxxp",
        "outputId": "42923d40-6e20-4142-b8df-4ba5ac091f35"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Environment setup check completed. Required libraries are installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 2: Helper Functions ---\n",
        "\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    \"\"\"Calculate the great circle distance in km between two points on the earth.\"\"\"\n",
        "    R = 6371  # Earth radius in kilometers\n",
        "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "    dlon = lon2 - lon1\n",
        "    dlat = lat2 - lat1\n",
        "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "    return R * c\n",
        "\n",
        "def fetch_realtime_weather(latitude, longitude, api_key):\n",
        "    \"\"\"Fetches real-time weather data for a given location using OpenWeatherMap API.\"\"\"\n",
        "    if api_key == \"YOUR_OPENWEATHERMAP_API_KEY\":\n",
        "        print(\"Warning: API Key is a placeholder. Skipping real-time fetch.\")\n",
        "        return 25.0, 'Clear', 5.0 # Return fallback values for simulation\n",
        "\n",
        "    try:\n",
        "        url = f\"http://api.openweathermap.org/data/2.5/weather?lat={latitude}&lon={longitude}&appid={api_key}&units=metric\"\n",
        "        response = requests.get(url, timeout=5)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        temp = data['main']['temp']\n",
        "        weather_main = data['weather'][0]['main']\n",
        "        wind_speed = data['wind']['speed']\n",
        "\n",
        "        print(f\"Weather fetched successfully: {weather_main}, {temp}째C\")\n",
        "        return temp, weather_main, wind_speed\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching weather data: {e}\")\n",
        "        return np.nan, 'Unknown', np.nan"
      ],
      "metadata": {
        "id": "J6vJ8BE1d2OL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 3: Dataset Creation and Feature Engineering ---\n",
        "\n",
        "def get_or_create_dataset():\n",
        "    \"\"\"Simulates loading and initial feature engineering on a historical dataset.\"\"\"\n",
        "    print(\"... Loading or creating simulated historical dataset ...\")\n",
        "\n",
        "    # 1. Simulate Historical Data\n",
        "    np.random.seed(42) # For reproducibility\n",
        "    data = {\n",
        "        'Order_ID': range(500), # Increased sample size\n",
        "        'Restaurant_lat': np.random.uniform(28.5, 28.7, 500),\n",
        "        'Restaurant_lon': np.random.uniform(77.1, 77.3, 500),\n",
        "        'Delivery_lat': np.random.uniform(28.5, 28.7, 500),\n",
        "        'Delivery_lon': np.random.uniform(77.1, 77.3, 500),\n",
        "        'Order_Placed_Time': pd.to_datetime(pd.date_range('2025-01-01', periods=500, freq='4H')),\n",
        "        'Initial_Estimate_Min': np.random.randint(25, 45, 500),\n",
        "        'Actual_Delivery_Time_Min': np.random.randint(20, 70, 500),\n",
        "        'preparation_time_min': np.random.randint(10, 30, 500),\n",
        "        'restaurant_rating': np.random.uniform(3.0, 5.0, 500).round(1),\n",
        "        'delivery_person_rating': np.random.uniform(4.0, 5.0, 500).round(1),\n",
        "        'Road_Traffic_Density': np.random.choice(['Low', 'Medium', 'High', 'Jam'], 500, p=[0.4, 0.3, 0.2, 0.1]),\n",
        "        'Weather_Condition': np.random.choice(['Clear', 'Rainy', 'Foggy', 'Stormy'], 500, p=[0.7, 0.2, 0.05, 0.05])\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # 2. Feature Engineering\n",
        "    df['delivery_distance_km'] = haversine(\n",
        "        df['Restaurant_lat'], df['Restaurant_lon'],\n",
        "        df['Delivery_lat'], df['Delivery_lon']\n",
        "    )\n",
        "\n",
        "    # Time-based features\n",
        "    df['order_hour'] = df['Order_Placed_Time'].dt.hour\n",
        "    df['day_of_week'] = df['Order_Placed_Time'].dt.day_name()\n",
        "\n",
        "    # Cyclic features for hour\n",
        "    df['sin_hour'] = np.sin(2 * np.pi * df['order_hour'] / 24)\n",
        "    df['cos_hour'] = np.cos(2 * np.pi * df['order_hour'] / 24)\n",
        "\n",
        "    # Target Variable\n",
        "    df['is_late'] = np.where(\n",
        "        df['Actual_Delivery_Time_Min'] > (df['Initial_Estimate_Min'] + THRESHOLD_MIN),\n",
        "        1,\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # Add simulated weather for training data (as historical data won't use the live API)\n",
        "    df['current_temp_c'] = np.random.uniform(15, 35, 500)\n",
        "\n",
        "    final_features = [\n",
        "        'delivery_distance_km', 'preparation_time_min', 'restaurant_rating',\n",
        "        'delivery_person_rating', 'Road_Traffic_Density', 'Weather_Condition',\n",
        "        'sin_hour', 'cos_hour'\n",
        "    ]\n",
        "\n",
        "    df_train = df[final_features + ['current_temp_c', 'is_late']].copy()\n",
        "\n",
        "    print(f\"Dataset created with {len(df_train)} samples. Target variable 'is_late' distribution:\")\n",
        "    print(df_train['is_late'].value_counts(normalize=True))\n",
        "    return df_train, final_features"
      ],
      "metadata": {
        "id": "MsE0wNvQd5aF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 4: Preprocessing Functions ---\n",
        "\n",
        "def create_preprocessing_pipeline(feature_list):\n",
        "    \"\"\"Creates a scikit-learn ColumnTransformer for preprocessing.\"\"\"\n",
        "\n",
        "    numeric_features = [\n",
        "        'delivery_distance_km', 'preparation_time_min', 'restaurant_rating',\n",
        "        'delivery_person_rating', 'current_temp_c', 'sin_hour', 'cos_hour'\n",
        "    ]\n",
        "    categorical_features = [\n",
        "        'Road_Traffic_Density', 'Weather_Condition'\n",
        "    ]\n",
        "\n",
        "    # Create preprocessing steps\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Combine transformers\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, [f for f in numeric_features if f in feature_list or f == 'current_temp_c']),\n",
        "            ('cat', categorical_transformer, [f for f in categorical_features if f in feature_list])\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "\n",
        "    print(\"Preprocessing pipeline (Scaling + One-Hot Encoding) created.\")\n",
        "    return preprocessor\n",
        "\n",
        "def perform_preprocessing(df_train, feature_list, preprocessor):\n",
        "    \"\"\"Splits data and prepares for the full pipeline.\"\"\"\n",
        "\n",
        "    # Include 'current_temp_c' in X for training as it's a numeric feature\n",
        "    X = df_train[feature_list + ['current_temp_c']].copy()\n",
        "    y = df_train['is_late']\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"Data split: Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "pQrPgVS1eGkW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 5: Model Training, Evaluation, and Saving ---\n",
        "\n",
        "def train_and_evaluate_model(X_train, X_test, y_train, y_test, preprocessor):\n",
        "    \"\"\"Defines, trains, and evaluates the final ML pipeline.\"\"\"\n",
        "\n",
        "    # Model Choice: XGBoost Hyperparameters\n",
        "    XGB_PARAMS = {\n",
        "        'objective': 'binary:logistic',\n",
        "        'n_estimators': 300,\n",
        "        'learning_rate': 0.05,\n",
        "        'max_depth': 5,\n",
        "        'subsample': 0.7,\n",
        "        'colsample_bytree': 0.7,\n",
        "        'random_state': 42,\n",
        "        'use_label_encoder': False,\n",
        "        'eval_metric': 'logloss'\n",
        "    }\n",
        "\n",
        "    model = XGBClassifier(**XGB_PARAMS)\n",
        "\n",
        "    # Create the full ML pipeline\n",
        "    full_pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "\n",
        "    print(\"... Training XGBoost model ...\")\n",
        "    start_time = time.time()\n",
        "    full_pipeline.fit(X_train, y_train)\n",
        "    end_time = time.time()\n",
        "    print(f\"Training completed in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "    # Evaluation\n",
        "    y_pred = full_pipeline.predict(X_test)\n",
        "    y_proba = full_pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    print(\"\\n--- Model Evaluation (Test Set) ---\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(f\"ROC AUC Score: {roc_auc_score(y_test, y_proba):.4f}\")\n",
        "\n",
        "    # Save the model to Google Colab instance storage\n",
        "    joblib.dump(full_pipeline, 'late_delivery_predictor_model.pkl')\n",
        "    print(\"Model saved as 'late_delivery_predictor_model.pkl'.\")\n",
        "\n",
        "    return full_pipeline"
      ],
      "metadata": {
        "id": "Ng0XK0b6eJba"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 6: Real-Time Dashboard (Conceptual & Demonstration) ---\n",
        "\n",
        "def create_real_time_dashboard(model_pipeline, features):\n",
        "    \"\"\"\n",
        "    Provides the conceptual code for the Streamlit dashboard\n",
        "    and demonstrates a real-time prediction using the saved model.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n--- Conceptual Streamlit Dashboard Code ---\")\n",
        "    print(\"To run the full dashboard, save the following code as 'app.py' and execute 'streamlit run app.py' locally.\")\n",
        "\n",
        "    # (The actual Streamlit code structure is omitted here but provided in the original output documentation)\n",
        "    # The output is focused on demonstrating the prediction function itself.\n",
        "\n",
        "    # --- Real-Time Prediction Demonstration ---\n",
        "    print(\"\\n--- Real-Time Prediction Demonstration ---\")\n",
        "\n",
        "    # 1. Define a New Order (Input Data)\n",
        "    NEW_ORDER_DATA = {\n",
        "        'Restaurant_lat': 28.60,\n",
        "        'Restaurant_lon': 77.15,\n",
        "        'Delivery_lat': 28.70,\n",
        "        'Delivery_lon': 77.25,\n",
        "        'preparation_time_min': 25,\n",
        "        'restaurant_rating': 4.2,\n",
        "        'delivery_person_rating': 4.9,\n",
        "    }\n",
        "\n",
        "    # 2. Feature Engineering\n",
        "    current_time = pd.Timestamp.now(tz='Asia/Kolkata')\n",
        "    order_hour = current_time.hour\n",
        "\n",
        "    delivery_distance_km = haversine(\n",
        "        NEW_ORDER_DATA['Restaurant_lat'], NEW_ORDER_DATA['Restaurant_lon'],\n",
        "        NEW_ORDER_DATA['Delivery_lat'], NEW_ORDER_DATA['Delivery_lon']\n",
        "    )\n",
        "\n",
        "    # Simulate Real-time Dynamic Features\n",
        "    temp, weather_main, wind_speed = fetch_realtime_weather(\n",
        "        NEW_ORDER_DATA['Delivery_lat'],\n",
        "        NEW_ORDER_DATA['Delivery_lon'],\n",
        "        WEATHER_API_KEY\n",
        "    )\n",
        "\n",
        "    # Traffic Density Simulation based on time (for demonstration)\n",
        "    traffic = 'Jam' if 17 <= order_hour <= 21 else 'High' if 12 <= order_hour <= 14 else 'Medium'\n",
        "\n",
        "    sin_hour = np.sin(2 * np.pi * order_hour / 24)\n",
        "    cos_hour = np.cos(2 * np.pi * order_hour / 24)\n",
        "\n",
        "    # 3. Create DataFrame (must match the features and order used in the pipeline)\n",
        "    input_data = pd.DataFrame({\n",
        "        'delivery_distance_km': [delivery_distance_km],\n",
        "        'preparation_time_min': [NEW_ORDER_DATA['preparation_time_min']],\n",
        "        'restaurant_rating': [NEW_ORDER_DATA['restaurant_rating']],\n",
        "        'delivery_person_rating': [NEW_ORDER_DATA['delivery_person_rating']],\n",
        "        'Road_Traffic_Density': [traffic],\n",
        "        'Weather_Condition': [weather_main],\n",
        "        'sin_hour': [sin_hour],\n",
        "        'cos_hour': [cos_hour],\n",
        "        'current_temp_c': [temp if not np.isnan(temp) else 25.0]\n",
        "    })\n",
        "\n",
        "    # 4. Predict\n",
        "    prediction_proba = model_pipeline.predict_proba(input_data)[:, 1][0] * 100\n",
        "\n",
        "    # 5. Output\n",
        "    print(f\"\\n--- Prediction Result ---\")\n",
        "    print(f\"Distance: {delivery_distance_km:.2f} km\")\n",
        "    print(f\"Current Traffic: {traffic}\")\n",
        "    print(f\"Weather: {weather_main}, {temp}째C\")\n",
        "    print(f\"**Probability of Being Late:** {prediction_proba:.2f}%\")\n",
        "    if prediction_proba > 50:\n",
        "        print(\"Conclusion: High risk of late delivery (Predicted Late).\")\n",
        "    else:\n",
        "        print(\"Conclusion: Low risk of late delivery (Predicted On-Time).\")"
      ],
      "metadata": {
        "id": "d2UWwSWweMIw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 7: Execute the Full Pipeline ---\n",
        "\n",
        "# Step 1 & 2: Get Data and Features\n",
        "df_train, feature_list = get_or_create_dataset()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Step 3: Preprocessing Setup\n",
        "preprocessor = create_preprocessing_pipeline(feature_list)\n",
        "X_train, X_test, y_train, y_test = perform_preprocessing(df_train, feature_list, preprocessor)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Step 4: Train, Evaluate, and Save Model\n",
        "trained_pipeline = train_and_evaluate_model(X_train, X_test, y_train, y_test, preprocessor)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Step 5: Real-time Prediction Demonstration (using the trained model)\n",
        "create_real_time_dashboard(trained_pipeline, feature_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LWpnomseQiD",
        "outputId": "b2dbf933-15e3-4ee3-fcff-a4b91b6a465e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "... Loading or creating simulated historical dataset ...\n",
            "Dataset created with 500 samples. Target variable 'is_late' distribution:\n",
            "is_late\n",
            "1    0.5\n",
            "0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "==================================================\n",
            "\n",
            "Preprocessing pipeline (Scaling + One-Hot Encoding) created.\n",
            "Data split: Train size: 400, Test size: 100\n",
            "\n",
            "==================================================\n",
            "\n",
            "... Training XGBoost model ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1424171555.py:15: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  'Order_Placed_Time': pd.to_datetime(pd.date_range('2025-01-01', periods=500, freq='4H')),\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [11:03:10] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 0.65 seconds.\n",
            "\n",
            "--- Model Evaluation (Test Set) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.58      0.57        50\n",
            "           1       0.57      0.56      0.57        50\n",
            "\n",
            "    accuracy                           0.57       100\n",
            "   macro avg       0.57      0.57      0.57       100\n",
            "weighted avg       0.57      0.57      0.57       100\n",
            "\n",
            "ROC AUC Score: 0.5356\n",
            "Model saved as 'late_delivery_predictor_model.pkl'.\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n",
            "--- Conceptual Streamlit Dashboard Code ---\n",
            "To run the full dashboard, save the following code as 'app.py' and execute 'streamlit run app.py' locally.\n",
            "\n",
            "--- Real-Time Prediction Demonstration ---\n",
            "Weather fetched successfully: Clouds, 26.15째C\n",
            "\n",
            "--- Prediction Result ---\n",
            "Distance: 14.79 km\n",
            "Current Traffic: Medium\n",
            "Weather: Clouds, 26.15째C\n",
            "**Probability of Being Late:** 81.42%\n",
            "Conclusion: High risk of late delivery (Predicted Late).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# This command initiates the download of the specified file to your local computer.\n",
        "files.download('late_delivery_predictor_model.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Zubs2k7nlMwz",
        "outputId": "a8829214-e4fc-407c-cf6d-b5df812b2d86"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f7eba1dc-25e1-49a9-a677-9d08a22f1be2\", \"late_delivery_predictor_model.pkl\", 518804)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}